services:
  postgres_people:
    image: postgres:13
    container_name: postgres_people
    environment:
      POSTGRES_USER: admin
      POSTGRES_PASSWORD: admin
      POSTGRES_DB: people
    ports:
      - "5432:5432"
    volumes:
      - postgres_people_data:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U admin -d people"]
      interval: 5s
      timeout: 5s
      retries: 5
    networks:
      - etl_net

  postgres_stage:
    image: postgres:13
    container_name: postgres_stage
    environment:
      POSTGRES_USER: admin
      POSTGRES_PASSWORD: admin
      POSTGRES_DB: stage_layer
    ports:
      - "5433:5432"
    volumes:
      - postgres_stage_data:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U admin -d stage_layer"]
      interval: 5s
      timeout: 5s
      retries: 5
    networks:
      - etl_net

  mongodb:
    image: mongo:6
    container_name: mongodb
    environment:
      MONGO_INITDB_ROOT_USERNAME: admin
      MONGO_INITDB_ROOT_PASSWORD: admin
    ports:
      - "27017:27017"
    volumes:
      - mongodb_data:/data/db
    healthcheck:
      test: ["CMD", "mongosh", "--eval", "db.adminCommand('ping')"]
      interval: 5s
      timeout: 5s
      retries: 5
    networks:
      - etl_net

  zookeeper:
    image: confluentinc/cp-zookeeper:7.3.0
    container_name: zookeeper
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
    ports:
      - "2181:2181"
    networks:
      - etl_net

  kafka:
    image: confluentinc/cp-kafka:7.3.0
    container_name: kafka
    depends_on:
      - zookeeper
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092,PLAINTEXT_HOST://localhost:29092
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
      KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: 'true'
    ports:
      - "9092:9092"
      - "29092:29092"
    healthcheck:
      test: ["CMD", "kafka-topics", "--bootstrap-server", "localhost:9092", "--list"]
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
      - etl_net

  clickhouse:
    image: clickhouse/clickhouse-server:23.3
    container_name: clickhouse
    ports:
      - "8123:8123"
      - "9000:9000"
    volumes:
      - ./clickhouse-config/users.xml:/etc/clickhouse-server/users.xml
      - clickhouse_data:/var/lib/clickhouse
    healthcheck:
      test: wget --no-verbose --tries=1 --spider http://localhost:8123/ping || exit 1
      interval: 5s
      timeout: 3s
      retries: 10
    networks:
      - etl_net
      
  airflow-webserver:
    build:
      context: ./app
      dockerfile: Dockerfile.airflow
    container_name: airflow-webserver
    depends_on:
      - airflow-scheduler
    environment:
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://admin:admin@postgres_stage:5432/stage_layer
    volumes:
      - ./pipeline/processing/batch:/opt/airflow/dags
    ports:
      - "8081:8080"
    command: webserver
    networks:
      - etl_net

  airflow-scheduler:
    build:
      context: .
      dockerfile: pipeline/processing/batch/Dockerfile.airflow
    container_name: airflow-scheduler
    depends_on:
      postgres_stage:
        condition: service_healthy
    environment:
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://admin:admin@postgres_stage:5432/stage_layer
    volumes:
      - ./pipeline/processing/batch:/opt/airflow/dags
    command: scheduler
    networks:
      - etl_net

  api-service:
    build:
      context: ./app
      dockerfile: Dockerfile.app
    container_name: api-service
    depends_on:
      - postgres_people
      - mongodb
      - kafka
    environment:
      - PYTHONUNBUFFERED=1
    ports:
      - "8000:8000"
    networks:
      - etl_net

  grafana:
    image: grafana/grafana:9.5.2
    container_name: grafana
    depends_on:
      - clickhouse
    ports:
      - "3000:3000"
    volumes:
      - grafana_data:/var/lib/grafana
      - ./grafana/provisioning:/etc/grafana/provisioning
    environment:
      - GF_INSTALL_PLUGINS=vertamedia-clickhouse-datasource
    networks:
      - etl_net

  kafka_exporter:
    image: danielqsj/kafka-exporter
    ports:
      - "9308:9308"
    environment:
      - KAFKA_URI=kafka:9092
    depends_on:
      - kafka
    networks:
      - etl_net

  clickhouse_exporter:
    image: flant/clickhouse-exporter
    container_name: clickhouse_exporter
    environment:
      - CLICKHOUSE_SERVER=clickhouse:8123
      - CLICKHOUSE_USER=default
      - CLICKHOUSE_PASSWORD="custom_password"
    ports:
      - "9363:9363"
    healthcheck:
      test: ["CMD", "wget", "-q", "http://localhost:9363/metrics", "-O", "-"]
      interval: 10s
      timeout: 5s
      retries: 3
    depends_on:
      clickhouse:
        condition: service_healthy
    networks:
      - etl_net

  alertmanager:
    image: prom/alertmanager:v0.25.0
    container_name: alertmanager
    volumes:
      - ./prometheus/alertmanager.yml:/etc/alertmanager/alertmanager.yml
    command:
      - '--config.file=/etc/alertmanager/alertmanager.yml'
    ports:
      - "9093:9093"
    networks:
      - etl_net

  prometheus:
    image: prom/prometheus:v2.46.0
    container_name: prometheus
    volumes:
      - ./prometheus/prometheus.yml:/etc/prometheus/prometheus.yml
      - ./prometheus/alerts:/etc/prometheus/alerts
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
    ports:
      - "9090:9090"
    networks:
      - etl_net

  data-generator:
    container_name: data-generator
    build:
      context: .
      dockerfile: data/Dockerfile.data-generator
    depends_on:
      postgres_people:
        condition: service_healthy
      mongodb:
        condition: service_healthy
      kafka:
        condition: service_healthy
    volumes:
      - ./data:/app/data
    networks:
      - etl_net

  data-processing:
    container_name: data-processing
    build:
      context: .
      dockerfile: pipeline/Dockerfile.data-processing
    depends_on:
      postgres_stage:
        condition: service_healthy
      kafka:
        condition: service_healthy
      clickhouse:
        condition: service_healthy
    volumes:
      - ./data:/app/data
      - ./pipeline/stage_layer:/app/pipeline/stage_layer
      - ./pipeline/processing/streaming:/app/pipeline/processing/streaming
      - ./pipeline/processing/clickhouse_views:/app/pipeline/processing/clickhouse_views
    restart: always
    networks:
      - etl_net

  app:
      build: .
      ports:
        - "8000:8000"
      networks:
        - etl_net

volumes:
  postgres_people_data:
  postgres_stage_data:
  mongodb_data:
  clickhouse_data:
  grafana_data:

networks:
  etl_net:
    driver: bridge